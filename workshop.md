# Workshop on Evolutionary Computing and Explainable AI
## Description

Full details of the ECXAI workshop and schedule will be made available here at a future date.

## Call for papers
Explainable artificial intelligence has gained significant traction in the machine learning community in recent years because of the need to generate “explanations” of how these typically black-box tools operate that are accessible to a wide range of users. Nature-inspired optimisation techniques are also often black box in nature, and the attention of the explainability community has begun to consider explaining their operation too. Many of the processes that drive nature-inspired optimisers are stochastic and complex, presenting a barrier to understanding how solutions to a given optimisation problem have been generated.

Explainable optimisation can address some of the questions that arise during the use of an optimiser: Is the system biased? Has the problem been formulated correctly? Is the solution trustworthy and fair? By providing mechanisms that enable a decision maker to interrogate an optimiser and answer these questions trust is built with the system. On the other hand, many approaches to XAI in machine learning are based on search algorithms that interrogate or refine the model to be explained, and have the potential to draw on the expertise of the EC community. Furthermore, many of the broader questions (such as what kinds of explanation are most appealing or useful to end users) are faced by XAI researchers in general.

From an application perspective, important questions have arisen, for which XAI may be crucial: Is the system biased? Has the problem been formulated correctly? Is the solution trustworthy and fair? The goal of XAI and related research is to develop methods to interrogate AI processes with the aim of answering these questions. This can support decision makers while also building trust in AI decision-support through more readily understandable explanations.

We seek contributions on a range of topics related to this theme, including but not limited to:
- Interpretability vs explainability in EC and their quantification
- Landscape analysis and XAI
- Contributions of EC to XAI in general
- Use of EC to generate explainable/interpretable models
- XAI in real-world applications of EC
- Possible interplay between XAI and EC theory
- Applications of existing XAI methods to EC
- Novel XAI methods for EC
- Legal and ethical considerations
- Case studies / applications of EC & XAI technologies

Papers will be double blind reviewed by members of our technical programme committee.

Authors can submit short contributions including position papers of up to 4 pages and regular contributions of up to 8 pages following in each category the GECCO paper formatting guidelines. Software demonstrations will also be welcome. 

## Important dates
 - Submission opening: February 11, 2022
 - Submission deadline: April 11, 2022
 - Notification: April 25, 2022
 - Camera-ready: May 2, 2022
 - Presenter mandatory registration: May 2, 2022

## Organisers (in alphabetical order)
 - Jaume Bacardit, Newcastle University, UK, jaume.bacardit@newcastle.ac.uk
 - Alexander Brownlee, University of Stirling, UK, alexander.brownlee@stir.ac.uk
 - Stefano Cagnoni, University of Parma, Italy, cagnoni@ce.unipr.it
 - Giovanni Iacca, University of Trento, Italy, giovanni.iacca@unitn.it
 - John McCall, Robert Gordon University, UK, j.mccall@rgu.ac.uk
 - David Walker, University of Plymouth, UK, david.walker@plymouth.ac.uk

## Technical Programme Committee
- Mauro Castelli
- Matthew Craven
- Alberto Franzin
- Julie Jacques
- Ed Keedwell
- Benjamin Lacroix
- Eric Medvet
- Fabrício Olivetti de França
- Roman Šenkeřík
- Ryan Urbanowicz
- Marco Virgolin
- Sean Walton
- Ciprian Zavoianu
